{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the trainning of my second model, focusing on using a finetuned mdeberta (for multiligual purpose) with some extra feature engineering (made in the EDA_FE notebook).\n",
    "\n",
    "Here we are tokenizing promt and unique response (a/b) together for each row. The resulting embbeding is then coupled with some feature engineering and fed to a classification fc layer.\n",
    "\n",
    "The modelisation roughly looks like that :\n",
    "> tokenise and embbed [CLS] prompt [SEP] response_a [SEP]  \n",
    "> tokenise and embbed [CLS] prompt [SEP] response_b [SEP]  \n",
    "> Cat [Embbed A] + [Embbed B]  \n",
    "> [Transformer Output Embedding] + [Feature Vector]  \n",
    "\n",
    "Feature engineering are pretty simple and include:\n",
    "- Prompt-Response Similarity\n",
    "- Response Length\n",
    "- N-grams/Keywords\n",
    "- lexical diversity\n",
    "\n",
    "Each time creating a single float by substracting the result of a and b.\n",
    "\n",
    "\n",
    "Reguarding the learning rate, I manually tried different setup, the best i got so far was to lower the learning rate for the finetuning part, giving more learning impact for the FC layer, coupled with a linear warm-up/decay scheduler (5%). I should have done some kind of gridsearch for better hyperparameters (will do for future competition). Also i found some ppl using Gemma couple with LoRA having pretty good results. I should take a look on this for the future.\n",
    "\n",
    "This solution will be the building block for the next competition : https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle\n",
    "# install in dependencies :\n",
    "#!pip install -U KeyBERT\n",
    "#import sys \n",
    "#sys.path.append(\"/kaggle/input/sentence-transformers-2-4-0/sentence_transformers-2.4.0-py3-none-any.whl\") \n",
    "#import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\Kaggle\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import ModelsUtils as Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu118\n",
      "Torch is build with CUDA: True\n",
      "Torch device : cuda\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "print('Torch version:', torch.__version__)\n",
    "print('Torch is build with CUDA:', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Torch device : {device}')\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence_length = 64\n",
    "sequence_length = 256\n",
    "#sequence_length = 512\n",
    "BATCH_SIZE = 1\n",
    "sample_size = 0.01      # Will only be taken if [MINI_RUN & BUILD_DATASET] are True\n",
    "EPOCHS = 1\n",
    "BUILD_DATASET = False#True # Will load from file pre-preprocessed data if False\n",
    "MINI_RUN = True         # Test run with very little data\n",
    "\n",
    "model_name = \"microsoft/mdeberta-v3-base\" # For multilingual purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = './kaggle/input/llm-classification-finetuning'\n",
    "CUSTOM_BASE_PATH = '../Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if BUILD_DATASET:\n",
    "    df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "else:\n",
    "    if MINI_RUN:\n",
    "        df = pd.read_csv(f'{CUSTOM_BASE_PATH}/train_preprocessed_mini.csv')\n",
    "    else:\n",
    "        df = pd.read_csv(f'{CUSTOM_BASE_PATH}/train_preprocessed_full.csv')\n",
    "\n",
    "# For kaggle\n",
    "#/kaggle/input/preprocessed-dataset-mini/train_preprocessed_mini.csv\n",
    "#/kaggle/input/train-preprocessed-full/train_preprocessed_full.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prompt'] = df['prompt'].astype(str)\n",
    "df['response_a'] = df['response_a'].astype(str)\n",
    "df['response_b'] = df['response_b'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Local \n",
    "# temp code to upload model on Kaggle (because not on Kaggle's pretrainned offline model list)\n",
    "if False:\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    save_path = 'deberta_v3_small_pretrained_model_pytorch_CPU'\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\Kaggle\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_data(prompt, response1, response2, max_length=sequence_length):\n",
    "    tokens_resp1 = tokenizer(\n",
    "        prompt,\n",
    "        response1,  # Pair of responses\n",
    "        #[response1, response2],  # Pair of responses\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokens_resp2 = tokenizer(\n",
    "        prompt,\n",
    "        response2,  # Pair of responses\n",
    "        #[response1, response2],  # Pair of responses\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids_resp1': tokens_resp1['input_ids'],\n",
    "        'attention_mask_resp1': tokens_resp1['attention_mask'],\n",
    "        'input_ids_resp2': tokens_resp2['input_ids'],\n",
    "        'attention_mask_resp2': tokens_resp2['attention_mask']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "dataset_train = Utils.ChatbotArenaDataset(df_train, tokenizer)\n",
    "dataloader_train = Utils.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_valid = Utils.ChatbotArenaDataset(df_valid, tokenizer)\n",
    "dataloader_valid = Utils.DataLoader(dataset_valid, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer\n",
    "model = Utils.PreferencePredictionModel(transformer_name=model_name, feature_dim=4, num_classes=3)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.transformer.parameters(), 'lr': 2e-6},     # Lower learning rate for transformer layers\n",
    "    {'params': model.feature_fc.parameters(), 'lr': 1e-3},      # Higher learning rate for custom layers\n",
    "], weight_decay=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_training_steps = len(dataloader_train) * EPOCHS\n",
    "num_warmup_steps = int(0.05 * num_training_steps)  # Warm up for 5% of total steps\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",  # Linear warm-up and decay\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [01:44<00:00,  4.95row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.914715599682628 val loss is better than previous 9.914715599682628, saving checkpoint epoch:  0\n",
      "Trainning Epoch 1, Accumulated Train Loss: 10.380057711937878\n",
      "Eval : Valid Loss: 9.914715599682628, Valid Accuracy : 0.41379310344827586\n",
      "Current learning rate: 0.0\n",
      "Current learning rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "Utils.train_model(model, dataloader_train, dataloader_valid, optimizer, lr_scheduler, device=device, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "In this notebook, we've achieved a good score with a small model and modest token length. Because of the complexity of the task and data, it hard to rapidly iterate and test different stuff. Also 30h free GPU from kaggle is very nice, but other ressources like Collab (expensive) might be a solution for faster iteration.\n",
    "\n",
    "There's plenty of room to improve. Here's how:\n",
    "\n",
    "- Higher token length (1024 ?)\n",
    "- Try bigger models like Gemma. I see a lot of good public score made with this model -> let's experiment\n",
    "- Better data handling, maybe filter some data, augment from other similar competition ?\n",
    "- some kind of grid search to find better parameters ?\n",
    "\n",
    "I stopped trying to improve this result as soon as i found out there was a timed competition of the same type with almost the same parameters. I simply continued to make this code evolve for another competition:\n",
    "\n",
    "https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
