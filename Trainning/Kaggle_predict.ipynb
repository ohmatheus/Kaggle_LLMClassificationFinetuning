{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\Kaggle\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from ModelsUtils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.46.3\n",
      "Sentence Transformers: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import transformers as trsf\n",
    "import sentence_transformers as sntc_trasfmr\n",
    "print(\"Transformers:\", trsf.__version__)\n",
    "print(\"Sentence Transformers:\", sntc_trasfmr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu118\n",
      "Torch is build with CUDA: True\n",
      "Torch device : cuda\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "print('Torch version:', torch.__version__)\n",
    "print('Torch is build with CUDA:', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Torch device : {device}')\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence_length = 64\n",
    "sequence_length = 256\n",
    "#sequence_length = 512\n",
    "BATCH_SIZE = 1\n",
    "sample_size = 0.01\n",
    "EPOCHS = 2\n",
    "BUILD_DATASET = False#True # Will load from file pre-preprocessed data if False\n",
    "MINI_RUN = True # Test run with very little data\n",
    "\n",
    "model_name = \"microsoft/mdeberta-v3-base\" # For multilingual purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = './kaggle/input/llm-classification-finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_df = pd.read_csv(f'{BASE_PATH}/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>I have three oranges today, I ate an orange ye...</td>\n",
       "      <td>You have two oranges today.</td>\n",
       "      <td>You still have three oranges. Eating an orange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>You are a mediator in a heated political debat...</td>\n",
       "      <td>Thank you for sharing the details of the situa...</td>\n",
       "      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>How to initialize the classification head when...</td>\n",
       "      <td>When you want to initialize the classification...</td>\n",
       "      <td>To initialize the classification head when per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  I have three oranges today, I ate an orange ye...   \n",
       "1   211333  You are a mediator in a heated political debat...   \n",
       "2  1233961  How to initialize the classification head when...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                        You have two oranges today.   \n",
       "1  Thank you for sharing the details of the situa...   \n",
       "2  When you want to initialize the classification...   \n",
       "\n",
       "                                          response_b  \n",
       "0  You still have three oranges. Eating an orange...  \n",
       "1  Mr Reddy and Ms Blue both have valid points in...  \n",
       "2  To initialize the classification head when per...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the first prompt and response\n",
    "test_df[\"prompt\"] = test_df.prompt.map(lambda x: ' '.join(eval(x.replace(\"null\",\"''\"))))\n",
    "test_df[\"response_a\"] = test_df.response_a.map(lambda x: ' '.join(eval(x.replace(\"null\",\"''\"))))\n",
    "test_df[\"response_b\"] = test_df.response_b.map(lambda x: ' '.join(eval(x.replace(\"null\", \"''\"))))\n",
    "\n",
    "# Show Sample\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prompt'] = test_df['prompt'].astype(str)\n",
    "test_df['response_a'] = test_df['response_a'].astype(str)\n",
    "test_df['response_b'] = test_df['response_b'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (re)Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>encode_fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>I have three oranges today, I ate an orange ye...</td>\n",
       "      <td>You have two oranges today.</td>\n",
       "      <td>You still have three oranges. Eating an orange...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>You are a mediator in a heated political debat...</td>\n",
       "      <td>Thank you for sharing the details of the situa...</td>\n",
       "      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             prompt  \\\n",
       "0  136060  I have three oranges today, I ate an orange ye...   \n",
       "1  211333  You are a mediator in a heated political debat...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                        You have two oranges today.   \n",
       "1  Thank you for sharing the details of the situa...   \n",
       "\n",
       "                                          response_b  encode_fail  \n",
       "0  You still have three oranges. Eating an orange...        False  \n",
       "1  Mr Reddy and Ms Blue both have valid points in...        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = test_df.apply(reencode, axis=1)  # Apply the make_pairs function to each row in df\n",
    "display(test_df.head(2))  # Display the first 2 rows of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering (for test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Response Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_length_features(df):\n",
    "    df['resp1_length'] = df['response_a'].apply(len)\n",
    "    df['resp2_length'] = df['response_b'].apply(len)\n",
    "    df['length_diff'] = df['resp1_length'] - df['resp2_length']  # Difference in lengths\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    tokens = text.split()  # Tokenize by whitespace\n",
    "    return len(set(tokens)) / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "def add_lexical_features(df):\n",
    "    df['resp1_lexical_div'] = df['response_a'].apply(lexical_diversity)\n",
    "    df['resp2_lexical_div'] = df['response_b'].apply(lexical_diversity)\n",
    "    df['lexical_div_diff'] = df['resp1_lexical_div'] - df['resp2_lexical_div']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis pipeline (ensure it's multilingual)\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", device=0)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    result = sentiment_analyzer(text[:512])  # Truncate to 512 tokens for BERT-based models\n",
    "    return result[0]['label']\n",
    "\n",
    "def add_sentiment_features(df):\n",
    "    df['resp1_sentiment'] = df['response_a'].apply(get_sentiment)\n",
    "    df['resp2_sentiment'] = df['response_b'].apply(get_sentiment)\n",
    "    # Convert sentiments to numeric scale (e.g., positive=1, neutral=0, negative=-1)\n",
    "    sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    df['resp1_sentiment_num'] = df['resp1_sentiment'].map(sentiment_map)\n",
    "    df['resp2_sentiment_num'] = df['resp2_sentiment'].map(sentiment_map)\n",
    "    df['sentiment_diff'] = df['resp1_sentiment_num'] - df['resp2_sentiment_num']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a multilingual sentence transformer model\n",
    "embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def calculate_similarity(prompt, response):\n",
    "    embeddings = embedder.encode([prompt, response])\n",
    "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "\n",
    "def add_similarity_features(df):\n",
    "    df['resp1_similarity'] = df.apply(lambda x: calculate_similarity(x['prompt'], x['response_a']), axis=1)\n",
    "    df['resp2_similarity'] = df.apply(lambda x: calculate_similarity(x['prompt'], x['response_b']), axis=1)\n",
    "    df['similarity_diff'] = df['resp1_similarity'] - df['resp2_similarity']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Question-Answer Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# Use KeyBERT for keyword extraction\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "def get_keyword_overlap(prompt, response):\n",
    "    prompt_keywords = set([kw[0] for kw in kw_model.extract_keywords(prompt)])\n",
    "    response_keywords = set([kw[0] for kw in kw_model.extract_keywords(response)])\n",
    "    overlap = len(prompt_keywords & response_keywords)\n",
    "    return overlap / len(prompt_keywords) if len(prompt_keywords) > 0 else 0\n",
    "\n",
    "def add_keyword_overlap_features(df):\n",
    "    df['resp1_keyword_overlap'] = df.apply(lambda x: get_keyword_overlap(x['prompt'], x['response_a']), axis=1)\n",
    "    df['resp2_keyword_overlap'] = df.apply(lambda x: get_keyword_overlap(x['prompt'], x['response_b']), axis=1)\n",
    "    df['keyword_overlap_diff'] = df['resp1_keyword_overlap'] - df['resp2_keyword_overlap']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Language-Specific Formality or Tone (TODO for each language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example with spaCy and third-party plugins\n",
    "# import spacy\n",
    "\n",
    "# # Load spaCy models for specific languages\n",
    "# nlp_en = spacy.load(\"en_core_web_sm\")  # English example\n",
    "\n",
    "# def detect_formality(text):\n",
    "#     doc = nlp_en(text)\n",
    "#     formality_score = sum(1 for token in doc if token.pos_ in [\"VERB\", \"ADV\"]) / len(doc)\n",
    "#     return formality_score if len(doc) > 0 else 0\n",
    "\n",
    "# def add_formality_features(df):\n",
    "#     df['resp1_formality'] = df['response_a'].apply(detect_formality)\n",
    "#     df['resp2_formality'] = df['response_b'].apply(detect_formality)\n",
    "#     df['formality_diff'] = df['resp1_formality'] - df['resp2_formality']\n",
    "#     return df\n",
    "\n",
    "# # wont add until all languages supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    return len(doc.ents)\n",
    "\n",
    "def add_ner_features(df, nlp_model):\n",
    "    df['resp1_entities'] = df['response_a'].apply(lambda x: count_entities(x, nlp_model))\n",
    "    df['resp2_entities'] = df['response_b'].apply(lambda x: count_entities(x, nlp_model))\n",
    "    df['entity_diff'] = df['resp1_entities'] - df['resp2_entities']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(df):\n",
    "    df = add_length_features(df)\n",
    "    df = add_lexical_features(df)\n",
    "    #df = add_sentiment_features(df)\n",
    "    df = add_similarity_features(df)\n",
    "    df = add_keyword_overlap_features(df)\n",
    "    #df = add_formality_features(df)\n",
    "    #df = add_ner_features(df)\n",
    "    return df\n",
    "\n",
    "# test will need preprocess no matter what, can't pre load them from kaggle\n",
    "test_df = extract_all_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\Kaggle\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ChatbotArenaDataset(test_df, tokenizer, max_length=sequence_length, test=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer\n",
    "model = PreferencePredictionModel(transformer_name=model_name, feature_dim=4, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best epoch\n",
    "checkpoint = torch.load(f'./Prebuilt/PreferencePredictionModel.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4150875210762024, 0.09427947551012039, 0.4906330406665802],\n",
       " [0.9964725375175476, 0.0015812570927664638, 0.0019460591720417142],\n",
       " [1.0, 1.1172556213523421e-09, 2.0532313627086296e-09]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.415088</td>\n",
       "      <td>9.427948e-02</td>\n",
       "      <td>4.906330e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.996473</td>\n",
       "      <td>1.581257e-03</td>\n",
       "      <td>1.946059e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.117256e-09</td>\n",
       "      <td>2.053231e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b    winner_tie\n",
       "0   136060        0.415088    9.427948e-02  4.906330e-01\n",
       "1   211333        0.996473    1.581257e-03  1.946059e-03\n",
       "2  1233961        1.000000    1.117256e-09  2.053231e-09"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = test_df[[\"id\"]].copy()\n",
    "sub_df[class_names] = prediction\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
